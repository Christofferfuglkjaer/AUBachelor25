{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from medmnist import PneumoniaMNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D , UpSampling2D, add, Input\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "from skimage.io import imshow\n",
    "import neptune\n",
    "# run = neptune.init_run(project='momkeybomkey/Federated',\n",
    "#                        api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjNDdlN2ZhNy00ZmJmLTQ4YjMtYTk0YS1lNmViZmZjZWRhNzUifQ==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def _collate_fn(data):\n",
    "        xs = []\n",
    "\n",
    "        for x, _ in data:\n",
    "            x = np.array(x).astype(np.float32) / 255.\n",
    "            xs.append(x)\n",
    "\n",
    "        return np.array(xs)\n",
    "\n",
    "def shuffle_iterator(iterator):\n",
    "    # iterator should have limited size\n",
    "    index = list(iterator)\n",
    "    total_size = len(index)\n",
    "    i = 0\n",
    "    random.shuffle(index)\n",
    "    result = []\n",
    "\n",
    "    while len(result) < total_size:\n",
    "        result.append(index[i])\n",
    "        i += 1\n",
    "        \n",
    "        if i >= total_size:\n",
    "            i = 0\n",
    "            random.shuffle(index)\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_loader(dataset, random):\n",
    "    total_size = len(dataset)\n",
    "    print('Size', total_size)\n",
    "    if random:\n",
    "        index_generator = shuffle_iterator(range(total_size))\n",
    "    else:\n",
    "        index_generator = list(range(total_size))\n",
    "\n",
    "    while True:\n",
    "        data = []\n",
    "\n",
    "        for _ in range(len(index_generator)):\n",
    "            idx = index_generator.pop()\n",
    "            data.append(dataset[idx])\n",
    "\n",
    "        return _collate_fn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\malth\\.medmnist\\pneumoniamnist_128.npz\n",
      "Using downloaded and verified file: C:\\Users\\malth\\.medmnist\\pneumoniamnist_128.npz\n",
      "Using downloaded and verified file: C:\\Users\\malth\\.medmnist\\pneumoniamnist_128.npz\n",
      "Size 4708\n",
      "Size 624\n",
      "Size 524\n",
      "Size 524\n"
     ]
    }
   ],
   "source": [
    "train = PneumoniaMNIST(split=\"train\", download=True, size=128)\n",
    "test = PneumoniaMNIST(split=\"test\", download=True, size=128)\n",
    "val = PneumoniaMNIST(split=\"val\", download=True, size=128)\n",
    "\n",
    "train_load = np.array(get_loader(train, True))\n",
    "test_load = np.array(get_loader(test, True))\n",
    "val_load = np.array(get_loader(val, False))\n",
    "\n",
    "val_noisy = (np.array(get_loader(val, True)) + 0.02 * tf.random.normal(val_load.shape))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Conv2D in module keras.src.layers.convolutional.conv2d:\n",
      "\n",
      "class Conv2D(keras.src.layers.convolutional.base_conv.BaseConv)\n",
      " |  Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), groups=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |\n",
      " |  2D convolution layer.\n",
      " |\n",
      " |  This layer creates a convolution kernel that is convolved with the layer\n",
      " |  input over a 2D spatial (or temporal) dimension (height and width) to\n",
      " |  produce a tensor of outputs. If `use_bias` is True, a bias vector is created\n",
      " |  and added to the outputs. Finally, if `activation` is not `None`, it is\n",
      " |  applied to the outputs as well.\n",
      " |\n",
      " |  Args:\n",
      " |      filters: int, the dimension of the output space (the number of filters\n",
      " |          in the convolution).\n",
      " |      kernel_size: int or tuple/list of 2 integer, specifying the size of the\n",
      " |          convolution window.\n",
      " |      strides: int or tuple/list of 2 integer, specifying the stride length\n",
      " |          of the convolution. `strides > 1` is incompatible with\n",
      " |          `dilation_rate > 1`.\n",
      " |      padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n",
      " |          `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n",
      " |          the left/right or up/down of the input. When `padding=\"same\"` and\n",
      " |          `strides=1`, the output has the same size as the input.\n",
      " |      data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n",
      " |          The ordering of the dimensions in the inputs. `\"channels_last\"`\n",
      " |          corresponds to inputs with shape\n",
      " |          `(batch_size, height, width, channels)`\n",
      " |          while `\"channels_first\"` corresponds to inputs with shape\n",
      " |          `(batch_size, channels, height, width)`. It defaults to the\n",
      " |          `image_data_format` value found in your Keras config file at\n",
      " |          `~/.keras/keras.json`. If you never set it, then it will be\n",
      " |          `\"channels_last\"`.\n",
      " |      dilation_rate: int or tuple/list of 2 integers, specifying the dilation\n",
      " |          rate to use for dilated convolution.\n",
      " |      groups: A positive int specifying the number of groups in which the\n",
      " |          input is split along the channel axis. Each group is convolved\n",
      " |          separately with `filters // groups` filters. The output is the\n",
      " |          concatenation of all the `groups` results along the channel axis.\n",
      " |          Input channels and `filters` must both be divisible by `groups`.\n",
      " |      activation: Activation function. If `None`, no activation is applied.\n",
      " |      use_bias: bool, if `True`, bias will be added to the output.\n",
      " |      kernel_initializer: Initializer for the convolution kernel. If `None`,\n",
      " |          the default initializer (`\"glorot_uniform\"`) will be used.\n",
      " |      bias_initializer: Initializer for the bias vector. If `None`, the\n",
      " |          default initializer (`\"zeros\"`) will be used.\n",
      " |      kernel_regularizer: Optional regularizer for the convolution kernel.\n",
      " |      bias_regularizer: Optional regularizer for the bias vector.\n",
      " |      activity_regularizer: Optional regularizer function for the output.\n",
      " |      kernel_constraint: Optional projection function to be applied to the\n",
      " |          kernel after being updated by an `Optimizer` (e.g. used to implement\n",
      " |          norm constraints or value constraints for layer weights). The\n",
      " |          function must take as input the unprojected variable and must return\n",
      " |          the projected variable (which must have the same shape). Constraints\n",
      " |          are not safe to use when doing asynchronous distributed training.\n",
      " |      bias_constraint: Optional projection function to be applied to the\n",
      " |          bias after being updated by an `Optimizer`.\n",
      " |\n",
      " |  Input shape:\n",
      " |\n",
      " |  - If `data_format=\"channels_last\"`:\n",
      " |      A 4D tensor with shape: `(batch_size, height, width, channels)`\n",
      " |  - If `data_format=\"channels_first\"`:\n",
      " |      A 4D tensor with shape: `(batch_size, channels, height, width)`\n",
      " |\n",
      " |  Output shape:\n",
      " |\n",
      " |  - If `data_format=\"channels_last\"`:\n",
      " |      A 4D tensor with shape: `(batch_size, new_height, new_width, filters)`\n",
      " |  - If `data_format=\"channels_first\"`:\n",
      " |      A 4D tensor with shape: `(batch_size, filters, new_height, new_width)`\n",
      " |\n",
      " |  Returns:\n",
      " |      A 4D tensor representing `activation(conv2d(inputs, kernel) + bias)`.\n",
      " |\n",
      " |  Raises:\n",
      " |      ValueError: when both `strides > 1` and `dilation_rate > 1`.\n",
      " |\n",
      " |  Example:\n",
      " |\n",
      " |  >>> x = np.random.rand(4, 10, 10, 128)\n",
      " |  >>> y = keras.layers.Conv2D(32, 3, activation='relu')(x)\n",
      " |  >>> print(y.shape)\n",
      " |  (4, 8, 8, 32)\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      Conv2D\n",
      " |      keras.src.layers.convolutional.base_conv.BaseConv\n",
      " |      keras.src.layers.layer.Layer\n",
      " |      keras.src.backend.tensorflow.layer.TFLayer\n",
      " |      keras.src.backend.tensorflow.trackable.KerasAutoTrackable\n",
      " |      tensorflow.python.trackable.autotrackable.AutoTrackable\n",
      " |      tensorflow.python.trackable.base.Trackable\n",
      " |      keras.src.ops.operation.Operation\n",
      " |      keras.src.saving.keras_saveable.KerasSaveable\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), groups=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.layers.convolutional.base_conv.BaseConv:\n",
      " |\n",
      " |  build(self, input_shape)\n",
      " |\n",
      " |  call(self, inputs)\n",
      " |\n",
      " |  compute_output_shape(self, input_shape)\n",
      " |\n",
      " |  convolution_op(self, inputs, kernel)\n",
      " |\n",
      " |  enable_lora(self, rank, a_initializer='he_uniform', b_initializer='zeros')\n",
      " |\n",
      " |  get_config(self)\n",
      " |      Returns the config of the object.\n",
      " |\n",
      " |      An object config is a Python dictionary (serializable)\n",
      " |      containing the information needed to re-instantiate it.\n",
      " |\n",
      " |  load_own_variables(self, store)\n",
      " |      Loads the state of the layer.\n",
      " |\n",
      " |      You can override this method to take full control of how the state of\n",
      " |      the layer is loaded upon calling `keras.models.load_model()`.\n",
      " |\n",
      " |      Args:\n",
      " |          store: Dict from which the state of the model will be loaded.\n",
      " |\n",
      " |  save_own_variables(self, store)\n",
      " |      Saves the state of the layer.\n",
      " |\n",
      " |      You can override this method to take full control of how the state of\n",
      " |      the layer is saved upon calling `model.save()`.\n",
      " |\n",
      " |      Args:\n",
      " |          store: Dict where the state of the model will be saved.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.layers.convolutional.base_conv.BaseConv:\n",
      " |\n",
      " |  kernel\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.layers.layer.Layer:\n",
      " |\n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Call self as a function.\n",
      " |\n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |\n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |\n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |\n",
      " |  add_loss(self, loss)\n",
      " |      Can be called inside of the `call()` method to add a scalar loss.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```python\n",
      " |      class MyLayer(Layer):\n",
      " |          ...\n",
      " |          def call(self, x):\n",
      " |              self.add_loss(ops.sum(x))\n",
      " |              return x\n",
      " |      ```\n",
      " |\n",
      " |  add_metric(self, *args, **kwargs)\n",
      " |\n",
      " |  add_variable(self, shape, initializer, dtype=None, trainable=True, autocast=True, regularizer=None, constraint=None, name=None)\n",
      " |      Add a weight variable to the layer.\n",
      " |\n",
      " |      Alias of `add_weight()`.\n",
      " |\n",
      " |  add_weight(self, shape=None, initializer=None, dtype=None, trainable=True, autocast=True, regularizer=None, constraint=None, aggregation='none', name=None)\n",
      " |      Add a weight variable to the layer.\n",
      " |\n",
      " |      Args:\n",
      " |          shape: Shape tuple for the variable. Must be fully-defined\n",
      " |              (no `None` entries). Defaults to `()` (scalar) if unspecified.\n",
      " |          initializer: Initializer object to use to populate the initial\n",
      " |              variable value, or string name of a built-in initializer\n",
      " |              (e.g. `\"random_normal\"`). If unspecified, defaults to\n",
      " |              `\"glorot_uniform\"` for floating-point variables and to `\"zeros\"`\n",
      " |              for all other types (e.g. int, bool).\n",
      " |          dtype: Dtype of the variable to create, e.g. `\"float32\"`. If\n",
      " |              unspecified, defaults to the layer's variable dtype\n",
      " |              (which itself defaults to `\"float32\"` if unspecified).\n",
      " |          trainable: Boolean, whether the variable should be trainable via\n",
      " |              backprop or whether its updates are managed manually. Defaults\n",
      " |              to `True`.\n",
      " |          autocast: Boolean, whether to autocast layers variables when\n",
      " |              accessing them. Defaults to `True`.\n",
      " |          regularizer: Regularizer object to call to apply penalty on the\n",
      " |              weight. These penalties are summed into the loss function\n",
      " |              during optimization. Defaults to `None`.\n",
      " |          constraint: Contrainst object to call on the variable after any\n",
      " |              optimizer update, or string name of a built-in constraint.\n",
      " |              Defaults to `None`.\n",
      " |          aggregation: Optional string, one of `None`, `\"none\"`, `\"mean\"`,\n",
      " |              `\"sum\"` or `\"only_first_replica\"`. Annotates the variable with\n",
      " |              the type of multi-replica aggregation to be used for this\n",
      " |              variable when writing custom data parallel training loops.\n",
      " |              Defaults to `\"none\"`.\n",
      " |          name: String name of the variable. Useful for debugging purposes.\n",
      " |\n",
      " |  build_from_config(self, config)\n",
      " |      Builds the layer's states with the supplied config dict.\n",
      " |\n",
      " |      By default, this method calls the `build(config[\"input_shape\"])` method,\n",
      " |      which creates weights based on the layer's input shape in the supplied\n",
      " |      config. If your config contains other information needed to load the\n",
      " |      layer's state, you should override this method.\n",
      " |\n",
      " |      Args:\n",
      " |          config: Dict containing the input shape associated with this layer.\n",
      " |\n",
      " |  compute_mask(self, inputs, previous_mask)\n",
      " |\n",
      " |  compute_output_spec(self, *args, **kwargs)\n",
      " |\n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |\n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |\n",
      " |  get_build_config(self)\n",
      " |      Returns a dictionary with the layer's input shape.\n",
      " |\n",
      " |      This method returns a config dict that can be used by\n",
      " |      `build_from_config(config)` to create all states (e.g. Variables and\n",
      " |      Lookup tables) needed by the layer.\n",
      " |\n",
      " |      By default, the config only contains the input shape that the layer\n",
      " |      was built with. If you're writing a custom layer that creates state in\n",
      " |      an unusual way, you should override this method to make sure this state\n",
      " |      is already created when Keras attempts to load its value upon model\n",
      " |      loading.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dict containing the input shape associated with the layer.\n",
      " |\n",
      " |  get_weights(self)\n",
      " |      Return the values of `layer.weights` as a list of NumPy arrays.\n",
      " |\n",
      " |  quantize(self, mode, type_check=True)\n",
      " |\n",
      " |  quantized_build(self, input_shape, mode)\n",
      " |\n",
      " |  quantized_call(self, *args, **kwargs)\n",
      " |\n",
      " |  set_weights(self, weights)\n",
      " |      Sets the values of `layer.weights` from a list of NumPy arrays.\n",
      " |\n",
      " |  stateless_call(self, trainable_variables, non_trainable_variables, *args, return_losses=False, **kwargs)\n",
      " |      Call the layer without any side effects.\n",
      " |\n",
      " |      Args:\n",
      " |          trainable_variables: List of trainable variables of the model.\n",
      " |          non_trainable_variables: List of non-trainable variables of the\n",
      " |              model.\n",
      " |          *args: Positional arguments to be passed to `call()`.\n",
      " |          return_losses: If `True`, `stateless_call()` will return the list of\n",
      " |              losses created during `call()` as part of its return values.\n",
      " |          **kwargs: Keyword arguments to be passed to `call()`.\n",
      " |\n",
      " |      Returns:\n",
      " |          A tuple. By default, returns `(outputs, non_trainable_variables)`.\n",
      " |              If `return_losses = True`, then returns\n",
      " |              `(outputs, non_trainable_variables, losses)`.\n",
      " |\n",
      " |      Note: `non_trainable_variables` include not only non-trainable weights\n",
      " |      such as `BatchNormalization` statistics, but also RNG seed state\n",
      " |      (if there are any random operations part of the layer, such as dropout),\n",
      " |      and `Metric` state (if there are any metrics attached to the layer).\n",
      " |      These are all elements of state of the layer.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```python\n",
      " |      model = ...\n",
      " |      data = ...\n",
      " |      trainable_variables = model.trainable_variables\n",
      " |      non_trainable_variables = model.non_trainable_variables\n",
      " |      # Call the model with zero side effects\n",
      " |      outputs, non_trainable_variables = model.stateless_call(\n",
      " |          trainable_variables,\n",
      " |          non_trainable_variables,\n",
      " |          data,\n",
      " |      )\n",
      " |      # Attach the updated state to the model\n",
      " |      # (until you do this, the model is still in its pre-call state).\n",
      " |      for ref_var, value in zip(\n",
      " |          model.non_trainable_variables, non_trainable_variables\n",
      " |      ):\n",
      " |          ref_var.assign(value)\n",
      " |      ```\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from keras.src.layers.layer.Layer:\n",
      " |\n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.layers.layer.Layer:\n",
      " |\n",
      " |  compute_dtype\n",
      " |      The dtype of the computations performed by the layer.\n",
      " |\n",
      " |  dtype\n",
      " |      Alias of `layer.variable_dtype`.\n",
      " |\n",
      " |  input_dtype\n",
      " |      The dtype layer inputs should be converted to.\n",
      " |\n",
      " |  losses\n",
      " |      List of scalar losses from `add_loss`, regularizers and sublayers.\n",
      " |\n",
      " |  metrics\n",
      " |      List of all metrics.\n",
      " |\n",
      " |  metrics_variables\n",
      " |      List of all metric variables.\n",
      " |\n",
      " |  non_trainable_variables\n",
      " |      List of all non-trainable layer state.\n",
      " |\n",
      " |      This extends `layer.non_trainable_weights` to include all state used by\n",
      " |      the layer including state for metrics and `SeedGenerator`s.\n",
      " |\n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weight variables of the layer.\n",
      " |\n",
      " |      These are the weights that should not be updated by the optimizer during\n",
      " |      training. Unlike, `layer.non_trainable_variables` this excludes metric\n",
      " |      state and random seeds.\n",
      " |\n",
      " |  path\n",
      " |      The path of the layer.\n",
      " |\n",
      " |      If the layer has not been built yet, it will be `None`.\n",
      " |\n",
      " |  quantization_mode\n",
      " |      The quantization mode of this layer, `None` if not quantized.\n",
      " |\n",
      " |  trainable_variables\n",
      " |      List of all trainable layer state.\n",
      " |\n",
      " |      This is equivalent to `layer.trainable_weights`.\n",
      " |\n",
      " |  trainable_weights\n",
      " |      List of all trainable weight variables of the layer.\n",
      " |\n",
      " |      These are the weights that get updated by the optimizer during training.\n",
      " |\n",
      " |  variable_dtype\n",
      " |      The dtype of the state (weights) of the layer.\n",
      " |\n",
      " |  variables\n",
      " |      List of all layer state, including random seeds.\n",
      " |\n",
      " |      This extends `layer.weights` to include all state used by the layer\n",
      " |      including `SeedGenerator`s.\n",
      " |\n",
      " |      Note that metrics variables are not included here, use\n",
      " |      `metrics_variables` to visit all the metric variables.\n",
      " |\n",
      " |  weights\n",
      " |      List of all weight variables of the layer.\n",
      " |\n",
      " |      Unlike, `layer.variables` this excludes metric state and random seeds.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.src.layers.layer.Layer:\n",
      " |\n",
      " |  dtype_policy\n",
      " |\n",
      " |  input_spec\n",
      " |\n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |\n",
      " |  trainable\n",
      " |      Settable boolean, whether this layer should be trainable or not.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.trackable.base.Trackable:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.ops.operation.Operation:\n",
      " |\n",
      " |  symbolic_call(self, *args, **kwargs)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.src.ops.operation.Operation:\n",
      " |\n",
      " |  from_config(config)\n",
      " |      Creates an operation from its config.\n",
      " |\n",
      " |      This method is the reverse of `get_config`, capable of instantiating the\n",
      " |      same operation from the config dictionary.\n",
      " |\n",
      " |      Note: If you override this method, you might receive a serialized dtype\n",
      " |      config, which is a `dict`. You can deserialize it as follows:\n",
      " |\n",
      " |      ```python\n",
      " |      if \"dtype\" in config and isinstance(config[\"dtype\"], dict):\n",
      " |          policy = dtype_policies.deserialize(config[\"dtype\"])\n",
      " |      ```\n",
      " |\n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the output of `get_config`.\n",
      " |\n",
      " |      Returns:\n",
      " |          An operation instance.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.ops.operation.Operation:\n",
      " |\n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a symbolic operation.\n",
      " |\n",
      " |      Only returns the tensor(s) corresponding to the *first time*\n",
      " |      the operation was called.\n",
      " |\n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |\n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |\n",
      " |      Only returns the tensor(s) corresponding to the *first time*\n",
      " |      the operation was called.\n",
      " |\n",
      " |      Returns:\n",
      " |          Output tensor or list of output tensors.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.saving.keras_saveable.KerasSaveable:\n",
      " |\n",
      " |  __reduce__(self)\n",
      " |      __reduce__ is used to customize the behavior of `pickle.pickle()`.\n",
      " |\n",
      " |      The method returns a tuple of two elements: a function, and a list of\n",
      " |      arguments to pass to that function.  In this case we just leverage the\n",
      " |      keras saving library.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Conv2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model():\n",
    "    n = 128\n",
    "    chan = 1\n",
    "    input_img = Input(shape=(n, n, chan))\n",
    "\n",
    "    l1 = Conv2D(32, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(input_img)\n",
    "    l2 = Conv2D(32, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l1)\n",
    "    l3 = MaxPooling2D(padding='same')(l2)\n",
    "\n",
    "    l4 = Conv2D(64, (3, 3),  padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l3)\n",
    "    l5 = Conv2D(64, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l4)\n",
    "    l6 = MaxPooling2D(padding='same')(l5)\n",
    "\n",
    "    # Decoder\n",
    "\n",
    "    l7 = UpSampling2D()(l6)\n",
    "    l8 = Conv2D(64, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10),)(l7)\n",
    "    l9 = Conv2D(64, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l8)\n",
    "    l10 = add([l5, l9])\n",
    "\n",
    "    l11 = UpSampling2D()(l10)\n",
    "    l12 = Conv2D(32, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l11)\n",
    "    l13 = Conv2D(32, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l12)\n",
    "    l14 = add([l13, l2])\n",
    "\n",
    "    # chan = 3, for RGB\n",
    "    decoded = Conv2D(chan, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l14)\n",
    "\n",
    "    return Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise(data, noise_factor = 0.2):\n",
    "    return data + noise_factor * tf.random.normal(shape=data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(orig, res):\n",
    "    return ((orig - res) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, n, noise_factor = 0.2):\n",
    "    m = len(data)\n",
    "    clean_split = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        clean_split.append(data[m * i // n: m * (i + 1) // n])\n",
    "\n",
    "    noisy_split = [generate_noise(x, noise_factor) for x in clean_split]\n",
    "    \n",
    "    return clean_split, noisy_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neptune_log(epoch, autoencoder):\n",
    "    print(\"Evaluation\")\n",
    "    run[\"evaluation/mse\"].append(autoencoder.evaluate(val_noisy, val_load[:10]))\n",
    "    run[f\"images/reconstructed_{epoch + 1}\"].upload(neptune.types.File.as_image(autoencoder.predict(val_noisy)[0]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neptune_val_images(autoencoder):\n",
    "    print(\"Final evaluation\")\n",
    "    decoded_imgs = autoencoder.predict(val_noisy)\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        run[f\"validation/original_{i}\"].upload(neptune.types.File.as_image(val_load[i]))\n",
    "        run[f\"validation/noisy_{i}\"].upload(neptune.types.File.as_image(val_noisy[i]))\n",
    "        run[f\"validation/reconstructed_{i}\"].upload(neptune.types.File.as_image(decoded_imgs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limited data\n",
    "# train_load = split_data(train_load, 5)\n",
    "# test_load = split_data(test_load, 5)\n",
    "\n",
    "# train_load = train_load[0][0]\n",
    "# test_load = test_load[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(n, epochs):\n",
    "    # Make overall model\n",
    "    autoencoder = setup_model()\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=losses.MeanSquaredError())\n",
    "\n",
    "    # try:\n",
    "    #     autoencoder.load_model(\"./Checkpoints/model.weights.h5\")\n",
    "    # except:\n",
    "    #     pass\n",
    "\n",
    "    # neptune_log(-1, autoencoder)\n",
    "    run[\"images/original\"].upload(neptune.types.File.as_image(val_load[0]))\n",
    "    run[\"images/noisy\"].upload(neptune.types.File.as_image(val_noisy[0]))\n",
    "\n",
    "    # Make n models\n",
    "    models = [setup_model() for _ in range(n)]\n",
    "    for model in models:\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=losses.MeanSquaredError())\n",
    "    \n",
    "\n",
    "    # Split data\n",
    "    train, train_noisy = split_data(train_load, n)\n",
    "    test, test_noisy = split_data(test_load, n)\n",
    "\n",
    "    # Get central weights\n",
    "    primary_weights = autoencoder.get_weights()\n",
    "    for model in models:\n",
    "        model.set_weights(primary_weights)\n",
    "    \n",
    "    # Train the networks\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "\n",
    "        for i, model in enumerate(models):\n",
    "            print(f\"Encoder {i + 1}\")\n",
    "            model.fit(train_noisy[i], train[i], batch_size=32, epochs=1, shuffle=True, validation_data=(test_noisy[i], test[i]))\n",
    "            run[f\"evaluation/encoder_{i + 1}/mse\"].append(model.evaluate(test_noisy[i], test[i]))\n",
    "\n",
    "        weights = [model.get_weights() for model in models]\n",
    "        weight_update = [0 for _ in primary_weights]\n",
    "\n",
    "        for weight in weights:\n",
    "            weight_update = [wu + (w - w0) / n for wu, w, w0 in zip(weight_update, weight, primary_weights)]\n",
    "\n",
    "        primary_weights = [w0 + wu for w0, wu in zip(primary_weights, weight_update)]\n",
    "\n",
    "        for model in models:\n",
    "            model.set_weights(primary_weights)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} completed\")\n",
    "        print(\"\")\n",
    "        \n",
    "        autoencoder.set_weights(primary_weights)\n",
    "        neptune_log(epoch, autoencoder)\n",
    "    \n",
    "    neptune_val_images(autoencoder)\n",
    "\n",
    "    autoencoder.save_weights(\"./Checkpoints/model_full.weights.h5\")\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Encoder 1\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 1s/step - loss: 0.0248 - val_loss: 0.0058\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 325ms/step - loss: 0.0059\n",
      "Epoch 1 completed\n",
      "\n",
      "Evaluation\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 10\n'y' sizes: 524\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m autoencoder \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 54\u001b[0m, in \u001b[0;36mrun_model\u001b[1;34m(n, epochs)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m     autoencoder\u001b[38;5;241m.\u001b[39mset_weights(primary_weights)\n\u001b[1;32m---> 54\u001b[0m     \u001b[43mneptune_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m neptune_val_images(autoencoder)\n\u001b[0;32m     58\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Checkpoints/model_full.weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m, in \u001b[0;36mneptune_log\u001b[1;34m(epoch, autoencoder)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mneptune_log\u001b[39m(epoch, autoencoder):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     run[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation/mse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_load\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m     run[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages/reconstructed_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupload(neptune\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mFile\u001b[38;5;241m.\u001b[39mas_image(autoencoder\u001b[38;5;241m.\u001b[39mpredict(val_noisy)[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\data_adapter_utils.py:115\u001b[0m, in \u001b[0;36mcheck_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    111\u001b[0m     sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[0;32m    113\u001b[0m     )\n\u001b[0;32m    114\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msizes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 10\n'y' sizes: 524\n"
     ]
    }
   ],
   "source": [
    "autoencoder = run_model(n = 1, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
