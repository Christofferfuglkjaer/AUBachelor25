{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from medmnist import PneumoniaMNIST\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from skimage.io import imshow\n",
    "from pytorch_msssim import ms_ssim\n",
    "import neptune\n",
    "# run = neptune.init_run(project='momkeybomkey/Federated',\n",
    "#                        api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjNDdlN2ZhNy00ZmJmLTQ4YjMtYTk0YS1lNmViZmZjZWRhNzUifQ=='\n",
    "#                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using CPU \n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('', 'Using GPU', '')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('', 'Using CPU', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collate_fn(data):\n",
    "        xs = []\n",
    "\n",
    "        for x, _ in data:\n",
    "            x = np.array(x).astype(np.float32) / 255.\n",
    "            xs.append([x])\n",
    "\n",
    "        return np.array(xs)\n",
    "\n",
    "def shuffle_iterator(iterator):\n",
    "    # iterator should have limited size\n",
    "    index = list(iterator)\n",
    "    total_size = len(index)\n",
    "    i = 0\n",
    "    random.shuffle(index)\n",
    "    result = []\n",
    "\n",
    "    while len(result) < total_size:\n",
    "        result.append(index[i])\n",
    "        i += 1\n",
    "        \n",
    "        if i >= total_size:\n",
    "            i = 0\n",
    "            random.shuffle(index)\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_loader(dataset, random):\n",
    "    total_size = len(dataset)\n",
    "    print('Size', total_size)\n",
    "    if random:\n",
    "        index_generator = shuffle_iterator(range(total_size))\n",
    "    else:\n",
    "        index_generator = list(range(total_size))\n",
    "\n",
    "    while True:\n",
    "        data = []\n",
    "\n",
    "        for _ in range(len(index_generator)):\n",
    "            idx = index_generator.pop()\n",
    "            data.append(dataset[idx])\n",
    "\n",
    "        return _collate_fn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise(data, noise_factor = 0.08):\n",
    "    data += noise_factor * (np.random.normal(size=data.shape) + np.random.poisson(data) / 255)\n",
    "    data = np.clip(data, 0., 1.)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def even_data(split):\n",
    "    min_size = min([len(data) for data in split])\n",
    "    return [data[:min_size] for data in split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    new_data = []\n",
    "    for split in np.array(data):\n",
    "        new_split = []\n",
    "        for im in split:\n",
    "            im -= im.min()\n",
    "            im /= im.max()\n",
    "            new_split.append(im)\n",
    "        new_data.append(new_split)\n",
    "    return np.array(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, n, noise_factor = 0.08):\n",
    "    m = len(data)\n",
    "    data_copy = copy.deepcopy(data)\n",
    "    clean_split = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        clean_split.append(data_copy[m * i // n: m * (i + 1) // n])\n",
    "\n",
    "    clean_split = even_data(clean_split)\n",
    "    noisy_split = copy.deepcopy(clean_split)\n",
    "    noisy_split = [generate_noise(x, noise_factor) for x in noisy_split]\n",
    "\n",
    "    clean_split = standardize(clean_split)\n",
    "    noisy_split = standardize(noisy_split)\n",
    "    \n",
    "    clean_split = torch.tensor(clean_split).float()\n",
    "    noisy_split = torch.tensor(noisy_split).float()\n",
    "\n",
    "    clean_split = clean_split.to(device)\n",
    "    noisy_split = noisy_split.to(device)\n",
    "\n",
    "    return clean_split, noisy_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/ciswhitemale/.medmnist/pneumoniamnist_224.npz\n",
      "Using downloaded and verified file: /Users/ciswhitemale/.medmnist/pneumoniamnist_224.npz\n",
      "Using downloaded and verified file: /Users/ciswhitemale/.medmnist/pneumoniamnist_224.npz\n",
      "Size 4708\n",
      "Size 624\n",
      "Size 524\n"
     ]
    }
   ],
   "source": [
    "train = PneumoniaMNIST(split=\"train\", download=True, size=224)\n",
    "test = PneumoniaMNIST(split=\"test\", download=True, size=224)\n",
    "val = PneumoniaMNIST(split=\"val\", download=True, size=224)\n",
    "\n",
    "train_loader = get_loader(train, random=True)\n",
    "test_loader = get_loader(test, random=True)\n",
    "\n",
    "val_loader = get_loader(val, random=False)\n",
    "\n",
    "val_clean, val_noisy = split_data(val_loader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, alpha = 0.84):\n",
    "    L1Loss = torch.nn.L1Loss()\n",
    "    L1 = L1Loss(recon_x, x)\n",
    "    MS_SSIM = 1 - ms_ssim(recon_x.unsqueeze(0), x.unsqueeze(0), data_range=1, size_average=True)\n",
    "    return alpha * MS_SSIM + (1 - alpha) * L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNR(y_true, y_pred):\n",
    "    mseloss = torch.nn.MSELoss()\n",
    "    return 20 * torch.log10(torch.max(y_true) / mseloss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neptune_log(epoch, model, loss, psnr):\n",
    "    print(\"Evaluation\")\n",
    "    run[\"evaluation/mse\"].append(loss)\n",
    "    run[\"evaluation/psnr\"].append(psnr) \n",
    "    run[f\"images/reconstructed_{epoch + 1}\"].upload(neptune.types.File.as_image(model(val_noisy[0][0])[0].cpu().detach().numpy()))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neptune_val_images(model):\n",
    "    print(\"Final evaluation\")\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        run[f\"validation/original_{i}\"].upload(neptune.types.File.as_image(val_clean[0][i][0].cpu().detach().numpy()))\n",
    "        run[f\"validation/noisy_{i}\"].upload(neptune.types.File.as_image(val_noisy[0][i][0].cpu().detach().numpy()))\n",
    "        run[f\"validation/reconstructed_{i}\"].upload(neptune.types.File.as_image(model(val_noisy[0][i])[0].cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limited data\n",
    "# train_load = split_data(train_load, 5)\n",
    "# test_load = split_data(test_load, 5)\n",
    "\n",
    "# train_load = train_load[0][0]\n",
    "# test_load = test_load[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, 3, stride=2, padding=1), # 112\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 32, 3, stride=2, padding=1), # 56\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, 3, stride=2, padding=1), # 28\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, 3, stride=2, padding=1), # 14\n",
    "            torch.nn.Flatten(0, -1),\n",
    "            torch.nn.Linear(64 * 14 * 14, 128)\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64 * 14 * 14),\n",
    "            torch.nn.Unflatten(0, (64, 14, 14)),\n",
    "            torch.nn.ConvTranspose2d(64, 64, 3, stride=2, padding=1, output_padding=1), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(n, epochs):\n",
    "    # Make overall model\n",
    "    autoencoder = AE()\n",
    "    autoencoder.to(device)\n",
    "\n",
    "    # try:\n",
    "    #     autoencoder.load_state_dict(torch.load(\"./Models/model_test.pth\"))\n",
    "    # except:\n",
    "    #     pass\n",
    "\n",
    "    # neptune_log(-1, autoencoder)\n",
    "    run[\"images/original\"].upload(neptune.types.File.as_image(val_clean[0][0][0].cpu().numpy()))\n",
    "    run[\"images/noisy\"].upload(neptune.types.File.as_image(val_noisy[0][0][0].cpu().numpy()))\n",
    "\n",
    "    # Make n models\n",
    "    models = [AE() for _ in range(n)]\n",
    "    \n",
    "\n",
    "    # Split data\n",
    "    train, train_noisy = split_data(train_loader, n)\n",
    "    test, test_noisy = split_data(test_loader, n)\n",
    "\n",
    "    # Get central weights\n",
    "    primary_weights = autoencoder.state_dict()\n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "        model.load_state_dict(primary_weights)\n",
    "    \n",
    "    # Train the networks\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "\n",
    "        for i, model in enumerate(models):\n",
    "            print(f\"Autoencoder {i + 1}\")\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "            total_loss = 0\n",
    "            total_psnr = 0\n",
    "\n",
    "            for j in range(len(train[i])):\n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_fn(model(train_noisy[i][j]), train[i][j])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            for noisy, clean in zip(test_noisy[i], test[i]):\n",
    "                loss = loss_fn(model(noisy), clean)\n",
    "                total_loss += loss.item()\n",
    "                total_psnr += PSNR(model(noisy), clean).item()\n",
    "\n",
    "            total_loss /= len(test[i])\n",
    "            total_psnr /= len(test[i])\n",
    "            run[f\"evaluation/autoencoder_{i + 1}/mse\"].append(total_loss)\n",
    "            run[f\"evaluation/autoencoder_{i + 1}/psnr\"].append(total_psnr)\n",
    "            print(f\"Loss {total_loss}\")\n",
    "            print(f\"PSNR {total_psnr}\")\n",
    "\n",
    "        # Aggregate the models\n",
    "        for key in primary_weights.keys():\n",
    "            primary_weights[key] = sum([model.state_dict()[key] for model in models]) / n\n",
    "\n",
    "        for model in models:\n",
    "            model.load_state_dict(primary_weights)\n",
    "\n",
    "        autoencoder.load_state_dict(primary_weights)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} completed\")\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_psnr = 0\n",
    "\n",
    "        for noisy, clean in zip(val_noisy[0], val_clean[0]):\n",
    "            loss = loss_fn(autoencoder(noisy), clean)\n",
    "            total_loss += loss.item()\n",
    "            total_psnr += PSNR(autoencoder(noisy), clean).item()\n",
    "        \n",
    "        total_loss /= len(val_noisy[0])\n",
    "        total_psnr /= len(val_noisy[0])\n",
    "\n",
    "        print(f\"Validation Loss {total_loss}\")\n",
    "        print(f\"Validation PSNR {total_psnr}\")\n",
    "        print(\"\")\n",
    "\n",
    "        neptune_log(epoch, autoencoder, total_loss, total_psnr)\n",
    "\n",
    "        torch.save(autoencoder.state_dict(), \"./Models/model_full.pth\")\n",
    "    \n",
    "    neptune_val_images(autoencoder)\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Autoencoder 1\n",
      "Loss 0.36861098197198683\n",
      "PSNR 31.98622219024166\n",
      "Autoencoder 2\n",
      "Loss 0.37805518279633216\n",
      "PSNR 32.19383622754005\n",
      "Autoencoder 3\n",
      "Loss 0.3781482455951552\n",
      "PSNR 32.34718019731583\n",
      "Autoencoder 4\n",
      "Loss 0.3790548572376851\n",
      "PSNR 31.44922645630375\n",
      "Autoencoder 5\n",
      "Loss 0.3675836604689398\n",
      "PSNR 32.476775323191\n",
      "Epoch 1 completed\n",
      "Validation Loss 0.3858573910218614\n",
      "Validation PSNR 30.673524528969335\n",
      "\n",
      "Evaluation\n",
      "\n",
      "Epoch 2\n",
      "Autoencoder 1\n",
      "Loss 0.2949131071086853\n",
      "PSNR 34.121075568660615\n",
      "Autoencoder 2\n",
      "Loss 0.3032991365799981\n",
      "PSNR 34.25566802486296\n",
      "Autoencoder 3\n",
      "Loss 0.2954783393971382\n",
      "PSNR 34.19003414338635\n",
      "Autoencoder 4\n",
      "Loss 0.3022055151241441\n",
      "PSNR 33.737448000138805\n",
      "Autoencoder 5\n",
      "Loss 0.29206307628943073\n",
      "PSNR 34.74777300127091\n",
      "Epoch 2 completed\n",
      "Validation Loss 0.29369893806581276\n",
      "Validation PSNR 33.79190320459031\n",
      "\n",
      "Evaluation\n",
      "\n",
      "Epoch 3\n",
      "Autoencoder 1\n",
      "Loss 0.26085883403016674\n",
      "PSNR 37.51900077635242\n",
      "Autoencoder 2\n",
      "Loss 0.2721539083267412\n",
      "PSNR 37.08473622414373\n",
      "Autoencoder 3\n",
      "Loss 0.2649183567733534\n",
      "PSNR 37.01345977475566\n",
      "Autoencoder 4\n",
      "Loss 0.2699931274739004\n",
      "PSNR 36.73930231217415\n",
      "Autoencoder 5\n",
      "Loss 0.2605529194397311\n",
      "PSNR 37.91081966892366\n",
      "Epoch 3 completed\n",
      "Validation Loss 0.25997041714669183\n",
      "Validation PSNR 36.791974595484845\n",
      "\n",
      "Evaluation\n",
      "\n",
      "Epoch 4\n",
      "Autoencoder 1\n",
      "Loss 0.24293964332149875\n",
      "PSNR 39.76900440646756\n",
      "Autoencoder 2\n",
      "Loss 0.25487899359676147\n",
      "PSNR 38.96425631738478\n",
      "Autoencoder 3\n",
      "Loss 0.2477845049673511\n",
      "PSNR 38.81558607470605\n",
      "Autoencoder 4\n",
      "Loss 0.252204327333358\n",
      "PSNR 38.666326707409276\n",
      "Autoencoder 5\n",
      "Loss 0.24101199329860748\n",
      "PSNR 39.95207772716399\n",
      "Epoch 4 completed\n",
      "Validation Loss 0.242092712945838\n",
      "Validation PSNR 38.99251386773495\n",
      "\n",
      "Evaluation\n",
      "\n",
      "Epoch 5\n",
      "Autoencoder 1\n",
      "Loss 0.23185315055231895\n",
      "PSNR 41.13655336441532\n",
      "Autoencoder 2\n",
      "Loss 0.2437360807652435\n",
      "PSNR 40.270186147382184\n",
      "Autoencoder 3\n",
      "Loss 0.23616588824699003\n",
      "PSNR 40.2380392166876\n",
      "Autoencoder 4\n",
      "Loss 0.24003039612885443\n",
      "PSNR 40.03454182224889\n",
      "Autoencoder 5\n",
      "Loss 0.23001094126412946\n",
      "PSNR 41.161727320763376\n",
      "Epoch 5 completed\n",
      "Validation Loss 0.23075262147165437\n",
      "Validation PSNR 40.557206543347306\n",
      "\n",
      "Evaluation\n",
      "\n",
      "Final evaluation\n"
     ]
    }
   ],
   "source": [
    "autoencoder = run_model(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/momkeybomkey/Federated/e/FED-28/metadata\n"
     ]
    }
   ],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system('shutdown -s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
